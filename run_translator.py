import os
import google.generativeai as genai
from dotenv import load_dotenv
import sys
from PIL import Image
import glob
import json

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
INPUT_FOLDER = "input_images"
OUTPUT_FOLDER = "output_translations"
FINAL_FILENAME = "full_chapter_translation.txt"
GLOSSARY_FILE = "glossary.json"

def load_glossary():
    if not os.path.exists(GLOSSARY_FILE):
        with open(GLOSSARY_FILE, 'w', encoding='utf-8') as f:
            json.dump({}, f)
        return {}
    try:
        with open(GLOSSARY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        return {}

def save_glossary(glossary_data):
    with open(GLOSSARY_FILE, 'w', encoding='utf-8') as f:
        json.dump(glossary_data, f, ensure_ascii=False, indent=4)

def translate_image(image_path, model, glossary, previous_page_translation):
    """
    ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³Ø±Ø¯ ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„ØµÙØ­Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ÙƒØ³ÙŠØ§Ù‚.
    """
    print(f"\n--- â³ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©: {os.path.basename(image_path)} ---")
    img = Image.open(image_path)
    
    glossary_text = "\n".join([f"- {k}: {v}" for k, v in glossary.items()])
    
    # --- Ø¨Ù†Ø§Ø¡ Ù‚Ø³Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ---
    context_section = ""
    if previous_page_translation:
        context_section = f"""
**Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù†ØªØ¨Ù‡ Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„!**
Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„ØµÙØ­Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©. Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙƒØ³ÙŠØ§Ù‚ Ù„ÙÙ‡Ù… Ø§Ù„Ù‚ØµØ© ÙˆØªØ±Ø¬Ù…Ø© Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­:
--- Ø¨Ø¯Ø§ÙŠØ© Ø³ÙŠØ§Ù‚ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ---
{previous_page_translation}
--- Ù†Ù‡Ø§ÙŠØ© Ø³ÙŠØ§Ù‚ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ---
"""

    # --- Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³Ø±Ø¯ ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„ ---
    translation_prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙˆÙ…ØªØ±Ø¬Ù… Ù…Ø§Ù†Ù‡ÙˆØ§ Ù…Ø­ØªØ±Ù. Ø§Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙˆÙ‚Ù… Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:

**Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆØ§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù…Ø³Ø±Ø¯ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø´ÙƒÙ„ Ø¥Ù„Ø²Ø§Ù…ÙŠ:**
--- Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ø³Ø±Ø¯ ---
{glossary_text}
--- Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø³Ø±Ø¯ ---
{context_section}
**Ø¨Ù‚ÙŠØ© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:**
Ù‚Ù… ÙÙ‚Ø· Ø¨Ù…Ø§ Ù‡Ùˆ Ù…Ø·Ù„ÙˆØ¨ Ø£Ø¯Ù†Ø§Ù‡ Ø¨Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ø£ÙŠ ÙƒÙ„Ø§Ù… Ø²Ø§Ø¦Ø¯ Ø£Ùˆ ØªÙˆØ¶ÙŠØ­Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©ØŒ ÙˆÙ„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù…Ø«Ù„ Ø§Ù„Ù†Ø¬ÙˆÙ… Ø£Ùˆ Ø§Ù„Ù…Ø§Ø±ÙƒØ¯Ø§ÙˆÙ† Ø£Ùˆ Ø£ÙŠ Ø´ÙƒÙ„ Ø¢Ø®Ø± Ù…Ù† Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ.

Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒÙˆØ±ÙŠ Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø®Ù„ ÙƒÙ„ ÙÙ‚Ø§Ø¹Ø© Ø­ÙˆØ§Ø± Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„. Ù„Ø§ ØªØ¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† ÙÙ‚Ø§Ø¹Ø§Øª Ù…Ø®ØªÙ„ÙØ©. Ø§Ù„ÙÙ‚Ø§Ø¹Ø© ØªÙØªØ±Ø¬Ù… ÙƒØ§Ù…Ù„Ø© Ø¨Ø¯ÙˆÙ† ÙØµÙ„ ÙˆÙ„Ø§ ØªØ¯Ù…Ø¬ Ø¨ÙŠÙ† Ø§Ù„ÙÙ‚Ø§Ø¹Ø§Øª.

ØªØ±Ø¬Ù… ÙƒÙ„ Ù†Øµ ÙƒÙˆØ±ÙŠ Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ¨Ø§Ù„ØªØ±ØªÙŠØ¨:

Ø§Ù„ÙƒÙˆØ±ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒÙˆØ±ÙŠ Ø§Ù„Ø£ØµÙ„ÙŠ Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø§Ù„Ø°ÙŠ Ø§Ø³ØªØ®Ø±Ø¬ØªÙ‡.
Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø­Ø±ÙÙŠØ©: Ø§ÙƒØªØ¨ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø­Ø±ÙÙŠØ© Ø¯ÙˆÙ† Ù‚Ø·Ø¹.
Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ø¯Ø¨ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: Ø§ÙƒØªØ¨ ØªØ±Ø¬Ù…Ø© Ø£Ø¯Ø¨ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø© ØµØ­ÙŠØ­Ø© Ù†Ø­ÙˆÙŠÙ‹Ø§ ÙˆÙ„ØºÙˆÙŠÙ‹Ø§ØŒ Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙˆØ§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ø¥ÙŠØµØ§Ù„ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø¨Ø£ÙØ¶Ù„ Ø´ÙƒÙ„ Ù„Ù„Ù‚Ø§Ø±Ø¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠ.

Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙˆØ±Ø© ÙƒØ¨ÙŠØ±Ø© ÙˆÙÙŠÙ‡Ø§ Ø£ÙƒØ«Ø± Ù…Ù† ÙÙ‚Ø§Ø¹Ø© Ø§Ø¬Ø¹Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ù‚. ØªØ±Ø¬Ù… ÙÙ‚Ø· Ø§Ù„ÙÙ‚Ø§Ø¹Ø§Øª ÙˆÙ„Ø§ ØªØªØ±Ø¬Ù… Ø§Ù„Ø£ØµÙˆØ§Øª.
Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ù†ØµÙ‹Ø§ Ø®Ø§Ø±Ø¬ ÙÙ‚Ø§Ø¹Ø§Øª Ø§Ù„Ø­ÙˆØ§Ø±ØŒ Ù‚Ù… Ø¨ØªØ±Ø¬Ù…ØªÙ‡ Ø¨Ù†ÙØ³ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø£Ø¹Ù„Ø§Ù‡ Ù„ÙƒÙ† Ø£Ø¶Ù ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø³Ø·Ø± Ø¹Ø¨Ø§Ø±Ø© Â«ØªÙ†Ø¨ÙŠÙ‡: Ù†Øµ Ø®Ø§Ø±Ø¬ ÙÙ‚Ø§Ø¹Ø©Â».
Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ù†Ø¸Ù…Ù‹Ø§ ÙˆÙ…ÙˆØ¬Ù‡Ù‹Ø§ Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø± ÙˆØ¨Ø¯ÙˆÙ† Ø£ÙŠ ØªÙ†Ø³ÙŠÙ‚ Ø£Ùˆ Ø¹Ù„Ø§Ù…Ø§Øª Ø®Ø§ØµØ©.
    """
    
    response = model.generate_content([translation_prompt, img])
    return response.text

def find_and_update_new_terms(text_to_analyze, model, glossary):
    # (Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±)
    print("--- ğŸ§  Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ØµØ·Ù„Ø­Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³Ø±Ø¯...")
    extraction_prompt = f"""
    Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ. Ø§Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ.
    Ù‡Ù„ ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ù†Øµ Ø¹Ù„Ù‰ Ø£ÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø´Ø®ØµÙŠØ§ØªØŒ Ø£Ù…Ø§ÙƒÙ†ØŒ Ø£Ùˆ Ù…ØµØ·Ù„Ø­Ø§Øª Ù…Ù‡Ù…Ø© Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø±Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠØŸ
    **Ø§Ù„Ù…Ø³Ø±Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠ:** {list(glossary.keys())}
    **Ø§Ù„Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„:** {text_to_analyze}
    **Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:** Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ù…ØµØ·Ù„Ø­Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©ØŒ Ø£Ø±Ø¬Ø¹Ù‡Ø§ ÙÙ‚Ø· Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø´ÙŠØ¦Ù‹Ø§ØŒ Ø£Ø±Ø¬Ø¹ {{}}.
    Ù…Ø«Ø§Ù„: {{"New Term 1": "Translation 1", "New Term 2": "Translation 2"}}
    """
    response = model.generate_content(extraction_prompt)
    try:
        clean_response = response.text.strip().replace('```json', '').replace('```', '')
        new_terms = json.loads(clean_response)
        if new_terms:
            updated_count = 0
            for term, translation in new_terms.items():
                if term not in glossary:
                    glossary[term] = translation
                    updated_count += 1
            if updated_count > 0:
                print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {updated_count} Ù…ØµØ·Ù„Ø­ Ø¬Ø¯ÙŠØ¯ ÙˆØ¥Ø¶Ø§ÙØªÙ‡Ø§ Ù„Ù„Ù…Ø³Ø±Ø¯.")
                save_glossary(glossary)
            else:
                print("--- Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ØµØ·Ù„Ø­Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©.")
        else:
            print("--- Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ØµØ·Ù„Ø­Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©.")
    except (json.JSONDecodeError, AttributeError) as e:
        print(f"[ØªØ­Ø°ÙŠØ±] Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©. Ø§Ù„Ø®Ø·Ø£: {e}")

def main():
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("[Ø®Ø·Ø£] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ GOOGLE_API_KEY.")
        sys.exit(1)
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.5-flash')
    print("âœ… Gemini API Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ°Ø§ÙƒØ±Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„Ø©.")

    glossary = load_glossary()
    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³Ø±Ø¯ ÙˆÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {len(glossary)} Ù…ØµØ·Ù„Ø­.")

    if not os.path.isdir(INPUT_FOLDER):
        print(f"[Ø®Ø·Ø£] Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ '{INPUT_FOLDER}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
        sys.exit(1)
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)

    image_paths = sorted(glob.glob(os.path.join(INPUT_FOLDER, '*.*')))
    if not image_paths:
        print(f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ ØµÙˆØ± ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ '{INPUT_FOLDER}'.")
        sys.exit(0)
    print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(image_paths)} ØµÙˆØ±Ø©. Ø³ØªØ¨Ø¯Ø£ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ±Ø¬Ù…Ø©...")

    all_translations_for_final_file = []
    
    # *** Ø§Ù„ØªØºÙŠÙŠØ± Ù‡Ù†Ø§: Ø³Ù†Ø®Ø²Ù† Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ***
    previous_page_full_translation = None

    for path in image_paths:
        # 1. ØªØ±Ø¬Ù…Ø© Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø±Ø¯ ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚
        translation = translate_image(path, model, glossary, previous_page_full_translation)
        
        # Ø­ÙØ¸ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ù†ÙØµÙ„Ø©
        base_name = os.path.basename(path)
        file_name_without_ext = os.path.splitext(base_name)[0]
        output_path = os.path.join(OUTPUT_FOLDER, f"{file_name_without_ext}.txt")
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(translation)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ù†ÙØµÙ„Ø© ÙÙŠ: {output_path}")
        
        # 2. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³Ø±Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        find_and_update_new_terms(translation, model, glossary)
        
        # *** Ø§Ù„ØªØºÙŠÙŠØ± Ù‡Ù†Ø§: ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© ***
        previous_page_full_translation = translation
        print(f"--- âœ… ØªÙ… Ø­ÙØ¸ Ø³ÙŠØ§Ù‚ Ø§Ù„ØµÙØ­Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ù…Ø±Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©.")
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ±Ø¬Ù…Ø§Øª Ù„Ù„Ù…Ù„Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        page_separator = f"\n\n--- Ù†Ù‡Ø§ÙŠØ© ØªØ±Ø¬Ù…Ø© ØµÙØ­Ø©: {base_name} ---\n\n"
        all_translations_for_final_file.append(translation + page_separator)

    final_output_path = os.path.join(OUTPUT_FOLDER, FINAL_FILENAME)
    with open(final_output_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(all_translations_for_final_file))
    
    print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¬Ù…Ø¹ Ù„Ù„ÙØµÙ„ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ: {final_output_path}")
    print("\nğŸ‰ğŸ‰ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª ØªØ±Ø¬Ù…Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± Ø¨Ù†Ø¬Ø§Ø­! ğŸ‰ğŸ‰ğŸ‰")

if __name__ == "__main__":
    main()
